{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example uses the MultiEURLEX dataset.\n",
    "### Log from running: Took 45 minutes on ml.g4dn.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to 'cpu' if the machine you are running Marqo on doesn't have a\n",
    "# Nvidia GPU\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing the marqo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marqo import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the huggingface datasets package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import other python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pprint\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be the name of the index:\n",
    "INDEX_NAME = \"my-multilingual-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this helps us see information about the HTTP requests\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Marqo client:\n",
    "mq = Client(\"http://localhost:8882\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index():\n",
    "    # Load the datasets. For this example we're just using the English and\n",
    "    # Deutsch validation splits:\n",
    "    dataset_en = load_dataset('multi_eurlex', 'en', split=\"validation\")\n",
    "    dataset_de = load_dataset('multi_eurlex', 'de', split=\"validation\")\n",
    "\n",
    "    # record the start time:\n",
    "    t0 = datetime.datetime.now()\n",
    "\n",
    "    # Create the index. The model we're using is multilingual:\n",
    "    mq.create_index(index_name=INDEX_NAME, model='stsb-xlm-r-multilingual')\n",
    "\n",
    "    # Let's break up large documents to make it easier to search:\n",
    "    MAX_TEXT_LENGTH = 100000\n",
    "\n",
    "    for ds, lang in [(dataset_en, \"en\"), (dataset_de, \"de\")]:\n",
    "        num_docs_in_dataset = len(ds)\n",
    "\n",
    "        for ii, doc in enumerate(ds):\n",
    "            dumped = json.dumps(doc)\n",
    "            # we'll set the doc ID to be the document's hash\n",
    "            doc_id = str(hash(dumped))\n",
    "\n",
    "            text_length = len(doc['text'])\n",
    "            split_size = MAX_TEXT_LENGTH//2\n",
    "            # break up the text of large documents:\n",
    "            if text_length > MAX_TEXT_LENGTH:\n",
    "                text_splits = [doc['text'][i: i + split_size] for i in range(0, text_length, split_size)]\n",
    "            else:\n",
    "                text_splits = [doc['text']]\n",
    "\n",
    "            for i, sub_doc in enumerate(text_splits):\n",
    "                # if a document is broken up, add the text's index to the end of the document:\n",
    "                qualified_id = f\"{doc_id}.{i}\" if len(text_splits) > 1 else doc_id\n",
    "                # create a dict to be posted\n",
    "                to_post = dict(\n",
    "                    [(k, v) if k != \"labels\" else (k, str(v)) for k, v in doc. items() if k != 'text']\n",
    "                    + [(\"_id\", qualified_id), (\"language\", lang), ('text', sub_doc)]\n",
    "                )\n",
    "                print(f\"doc number {ii} out of {num_docs_in_dataset} docs in dataset {lang}. \"\n",
    "                      f\"_id: {qualified_id}, celex_id: {doc['celex_id']}, \"\n",
    "                      f\"json to send size: {len(json.dumps(to_post))}\")\n",
    "                # Index the document. The device is set to 'cuda' to take\n",
    "                # advantage of the machine's GPU. If you don't have a GPU,\n",
    "                # change this argument to 'cpu'.\n",
    "                # We set auto_refresh to False which is optimal for indexing\n",
    "                # a lot of documents.\n",
    "                mq.index(index_name=INDEX_NAME).add_documents(\n",
    "                    documents=[to_post], device=DEVICE, auto_refresh=False\n",
    "                )\n",
    "    t1 = datetime.datetime.now()\n",
    "    print(f\"finished indexing. Started at {t0}. Finished at {t1}. Took {t1 - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(q):\n",
    "    # Set searchable_attributes to 'text', which ensures that Marqo just\n",
    "    # searches the 'text' field\n",
    "    result = mq.index(INDEX_NAME).search(q=q, searchable_attributes=['text'])\n",
    "    # Just print out the highlights, which makes the output easier to read\n",
    "    for res in result[\"hits\"]:\n",
    "        pprint.pprint(res[\"_highlights\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you finishing indexing, comment out the following line to prevent going through\n",
    "# the whole indexing process again.\n",
    "build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'my_search_query' with whatever text you want to search. In English or Deutsch!\n",
    "my_search_query = \"Laws about the fishing industry\"\n",
    "search(my_search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2ccb58c476f33ba3e3aee7ac07234ef6b8217ef24ad64d2a7d4fed1a57c1cd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
