# import random
# import time
# import unittest
# from concurrent.futures import ThreadPoolExecutor
# from queue import Queue
#
# import numpy as np
#
# from marqo.api.exceptions import EnvVarError
# from marqo.inference.inference_cache.marqo_inference_cache import MarqoInferenceCache
#
#
# class TestInferenceCache(unittest.TestCase):
#     """A test suite for the InferenceCache class outside marqo.s2_inference.s2_inference.vectorise function"""
#
#     def setUp(self):
#         self.cache_size = 10
#
#     def test_cache_initializationCacheType_success(self):
#         """Test if the cache initializes with the correct size and type."""
#         test_cases = [
#             {"cache_size": 10, "cache_type": "LRU", "expected": "LRU"},
#             {"cache_size": 10, "cache_type": "LFU", "expected": "LFU"}
#         ]
#         for test_case in test_cases:
#             with self.subTest(test_case):
#                 cache = MarqoInferenceCache(cache_size=test_case["cache_size"],
#                                             cache_type=test_case["cache_type"])
#                 self.assertEqual(test_case["cache_size"], cache.maxsize)
#                 self.assertTrue(isinstance(cache._cache,
#                                            MarqoInferenceCache._CACHE_TYPES_MAPPING[test_case["expected"]]))
#                 self.assertEqual(0, cache.currsize)
#
#     def test_inference_cache_generateKey(self):
#         model_cache_key = "model_cache_key"
#         content = "content"
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 key = cache._generate_key(model_cache_key, content)
#                 self.assertEqual(key, f"{model_cache_key}||{content}")
#
#     def test_cache_initializationCacheType_fail(self):
#         """Test if the cache raises an error for an invalid cache type."""
#         test_cases = [
#             {"cache_size": 10, "cache_type": "INVALID"},  # Invalid cache type
#             {"cache_size": 10, "cache_type": 1},  # Invalid cache type
#             {"cache_size": 1.4, "cache_type": "LFU"},  # Invalid cache size
#             {"cache_size": -1, "cache_type": "LRU"}  # Invalid cache size
#         ]
#         for test_case in test_cases:
#             with self.subTest(test_case):
#                 with self.assertRaises(EnvVarError):
#                     MarqoInferenceCache(cache_size=test_case["cache_size"], cache_type=test_case["cache_type"])
#
#     def test_cache_setAndGetItems(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 cache.set("key1", "content1", [1.0])
#                 self.assertTrue(("key1", "content1") in cache)
#                 self.assertEqual(cache.get("key1", "content1"), [1.0])
#
#     def test_cache_getNoneExistItems(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 self.assertIsNone(cache.get("non-existing-model-cache-key", "content"))
#                 self.assertEqual(cache.get("non-existing-model-cache-key", "content",
#                                            default=[2.0]), [2.0])
#
#     def test_cache_itemOverRide(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 cache.set("model-cache-key", "content", [1.0])
#                 cache.set("model-cache-key", "content", [2.0])
#                 self.assertEqual(cache.get("model-cache-key", "content"), [2.0])
#
#     def test_cache_evictionPolicy(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 # Fill up the cache
#                 for i in range(self.cache_size):
#                     cache.set("model-cache-key", f"content-{i}", [float(i)])
#
#                 if cache_type == "LRU":
#                     # Access the first key to update its LRU position
#                     cache.get("model-cache-key", f"content-{0}")
#                     # Adding a new key should evict the least recently used key, which is now "key1"
#                     evicted_key = ("model-cache-key", "content-1")
#                 else:  # LFU
#                     # Increase the access frequency of all but the last key
#                     for i in range(self.cache_size - 1):
#                         cache.get("model-cache-key", f"content-{i}")
#                     # Adding a new key should evict the least frequently used key, which is "key9"
#                     evicted_key = ("model-cache-key", f"content-{self.cache_size - 1}")
#
#                 cache.set("model-cache-key", "new", [100.0])
#                 self.assertTrue(("model-cache-key", "new") in cache)
#                 self.assertFalse(evicted_key in cache, f"{evicted_key} was not evicted under {cache_type} policy")
#
#     def test_cache_concurrentReads(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 cache.set("test-model-cache-key", "test-content", [1.0])
#                 # Use ThreadPoolExecutor to simulate concurrent reads
#                 with ThreadPoolExecutor(max_workers=10) as executor:
#                     futures = [executor.submit(lambda: cache.get("test-model-cache-key",
#                                                                  "test-content")) for _ in range(10)]
#                     results = [future.result() for future in futures]
#
#                 # Verify all reads were successful and returned the correct data
#                 self.assertTrue(all(result == [1.0] for result in results))
#
#     def test_cache_concurrentWrites(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 # Use ThreadPoolExecutor to simulate concurrent writes
#                 with ThreadPoolExecutor(max_workers=10) as executor:
#                     futures = [executor.submit(lambda i=i: cache.set("test-model-cache-key",
#                                                                      f"test-content-{i}",
#                                                                      [float(i)])) for i in range(10)]
#                     # Ensure all futures complete
#                     for future in futures:
#                         future.result()
#
#                 # Verify all writes were successful
#                 for i in range(10):
#                     self.assertEqual(cache.get("test-model-cache-key",
#                                                f"test-content-{i}"), [float(i)])
#
#     def test_cache_readWriteLock(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 cache.set("block-key", "block-content", [99.0])
#                 # Simulate a concurrent read and write to test the read-write lock behavior
#                 with ThreadPoolExecutor(max_workers=2) as executor:
#                     future_write = executor.submit(lambda: cache.set("block-key",
#                                                                      "block-content", [100.0]))
#                     time.sleep(0.1)  # Small delay to ensure write starts first
#                     future_read = executor.submit(lambda: cache.get("block-key",
#                                                                     "block-content"))
#
#                 # Ensure write has completed before read
#                 write_result = future_write.result()
#                 read_result = future_read.result()
#
#                 self.assertIsNone(write_result)  # set operation returns None
#                 self.assertEqual(read_result, [100.0], "Read did not return the updated value after write")
#
#     def test_cache_concurrentWritesToSameKey(self):
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 model_cache_key = "shared-model-cache-key"
#                 content = "shared-content"
#                 with ThreadPoolExecutor(max_workers=10) as executor:
#                     futures = [executor.submit(lambda value=i: cache.set(model_cache_key, content,
#                                                                          [value])) for i in range(10)]
#
#                     for future in futures:
#                         future.result()
#                     final_value = cache.get(model_cache_key, content)
#                     self.assertIn(final_value, [[i] for i in range(10)],
#                                   f"Final value {final_value} is not an expected value under {cache_type} policy")
#
#     def test_cache_isEnabled(self):
#         """Test if the cache is enabled or disabled based on the cache size."""
#         # Test with a positive cache size
#         cache = MarqoInferenceCache(cache_size=10, cache_type="LRU")
#         self.assertTrue(cache.is_enabled(), "Cache should be enabled with a positive cache_size.")
#
#         # Test with cache_size 0, which should disable the cache
#         cache = MarqoInferenceCache(cache_size=0, cache_type="LRU")
#         self.assertFalse(cache.is_enabled(), "Cache should be disabled with cache_size 0.")
#
#     def test_cache_threadSafety(self):
#         """Test if the cache is thread-safe by simulating concurrent reads and writes."""
#         DATA_DIMENSIONS = 768
#         SIZE = 16384
#         ITERATIONS = 100_000
#         FREQUENT_ACCESS_RATIO = 0.5
#         FREQUENT_ACCESS_SUBSET_SIZE = 5000
#         TOTAL_QUERY_SET_SIZE = 1_000_000
#
#         hits = Queue()
#         misses = Queue()
#         model_cache_key = "model_cache_key"
#
#         def read_write_cache(cache):
#             if random.random() < FREQUENT_ACCESS_RATIO:
#                 text = random.choice(frequent_texts)
#             else:
#                 text = random.choice(texts)
#             cache_value = cache.get(model_cache_key, text)
#             if cache_value is None:
#                 cache_value = np.random.rand(1, DATA_DIMENSIONS).astype(np.float32).tolist()
#                 cache.set(model_cache_key, text, cache_value)
#                 misses.put(1)
#                 return cache_value
#             else:
#                 hits.put(1)
#                 return cache_value
#
#         texts = [f"text{i} " * 5 for i in range(TOTAL_QUERY_SET_SIZE)]
#         frequent_texts = random.sample(texts, FREQUENT_ACCESS_SUBSET_SIZE)
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 test_cache = MarqoInferenceCache(cache_size=SIZE, cache_type="LRU")
#                 with ThreadPoolExecutor(max_workers=8) as executor:
#                     futures = [executor.submit(read_write_cache, test_cache) for _ in
#                                range(ITERATIONS)]
#                     result = [future.result() for future in futures]
#                     self.assertEqual(ITERATIONS, len(result))
#                     self.assertTrue(hits.qsize() > 0)
#                     self.assertTrue(misses.qsize() > 0)
#
#     def test_inference_cache_clear(self):
#         """Test if the cache clears all items."""
#         for cache_type in ['LRU', 'LFU']:
#             with self.subTest(cache_type=cache_type):
#                 cache = MarqoInferenceCache(cache_size=self.cache_size, cache_type=cache_type)
#                 cache.set("model-cache-key", "content", [1.0])
#                 cache.clear()
#                 self.assertEqual(cache.currsize, 0)
#                 self.assertIsNone(cache.get("model-cache-key", "content"))
#                 self.assertEqual(cache.maxsize, self.cache_size)
